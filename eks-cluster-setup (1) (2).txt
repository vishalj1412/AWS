EKS setup using EKSCTL, ALB Ingress Controller setup, Cluster Autoscaler setup and Sample Helm chart for Application deployment:
=====================================================================================================================

Prequisites:
------------
1) Install AWS CLI
      Linux,Windows - https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html
      
      Windows - Powershell - msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi
                              aws --version

2) Install eksctl
      Linux,Windows - https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html
	  
	  Linux:
	     curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
	     sudo mv /tmp/eksctl /usr/local/bin
	     eksctl version
	  
3) Install helm
      Linux:
         $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
         $ chmod 700 get_helm.sh
         $ ./get_helm.sh

4) Install kubectl
      Linux,Windows - https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html

      Linux:
	     Kubernetes 1.20 -  curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.20.4/2021-04-12/bin/linux/amd64/kubectl
		                chmod +x ./kubectl
                                mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
                                echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
                                kubectl version --short --client
							
5) Install Git
      Linux,Windows - https://www.atlassian.com/git/tutorials/install-git
	  
      Linux: 
	     $ sudo apt-get update
         $ sudo apt-get install git
         $ git --version
        

EKS setup using EKSCTL:
-----------------------

https://eksctl.io/usage/creating-and-managing-clusters/
Use required schema/options for custer config from this page - https://eksctl.io/usage/schema/

amiFamily - AmazonLinux2 (default), Ubuntu2004, Ubuntu1804

1) Create a file(ex: cluster-config.yaml) and paste below configurations. Change the VPC and Subnet id's in the file.

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: demo-cluster-eksctl-prod
  region: ap-south-1
  version: "1.21"
vpc:
  id: <vpc-id>
  subnets:
    public:
      public-1a:
        id: <subnet-id>
      public-1b:
        id: <subnet-id>
    private:
      private-1a:
        id: <subnet-id>
      private-1b:
        id: <subnet-id>
managedNodeGroups:
  - name: prod-v2
    amiFamily: AmazonLinux2
    instanceType: t2.medium
    ssh:
      publicKeyName: <keypairname>
    desiredCapacity: 1
    minSize: 1
    maxSize: 5
    privateNetworking: true
    volumeSize: 20
    volumeType: gp3
    subnets:
      - private-1a
      - private-1b
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      withAddonPolicies:
        imageBuilder: true
        autoScaler: true
        certManager: true
        ebs: true
        albIngress: true
        cloudWatch: true
		
2) Run the below eksctl command with the file.
 
      eksctl create cluster -f cluster-config.yaml
	  
	  
ALB Ingress Controller setup:
-----------------------------

https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/deploy/installation/

1) Add tags in subnets. Change the cluster name in the tags.

       Public:
           kubernetes.io/role/elb: 1
           kubernetes.io/cluster/${your-cluster-name}: owned
   
       Private:
           kubernetes.io/role/internal-elb: 1
           kubernetes.io/cluster/${your-cluster-name}: owned

2) Run below command to add eks chart
      helm repo add eks https://aws.github.io/eks-charts

3) Run below command to install the TargetGroupBinding CRDs
      kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
	  
4) Run below command to install ALB ingress controller. Change the cluster name in the command
      helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=<cluster-name>
		  


Sample Helm chart for Application deployment:
---------------------------------------------

1) Create a namespace and update namespace name in values.yaml

            kubectl create ns dev

2) Create a helm chart in the server

            helm create demo-helm-chart
					
3) Change the application name in our sample helm chart using the below sed command(replacing MAN with ThisIsTooLargePleaseJustify using below sed command)

            grep -RirwIl 'MAN' | xargs sed -i 's/\bMAN\b/ThisIsTooLargePleaseJustify/gI'  
			
3) Delete the follwing files in the chart and add our sample files or remove and paste our chart data in mentioned files.(values.yaml, deployment.yaml, service.yaml, hpa.yaml, ingress.yaml)  

 
     Values.yaml
     ------------
applicationname:
  replicaCount: 1

  meta:
    name: java-web-app-new
    label: java-web-app-new-label
    namespace: dev
  rollingupdate:
    maxunavailable: 1
    maxsurge: 1
    minreadyseconds: 50

  ports:
    containerport: 8080

  httpget:
    port: 8080
  http:
    initialdelayseconds: 50
    timeoutseconds: 2
    periodseconds: 15

  requests:
    cpu: 200m
    memory: 256Mi
  limits:
    cpu: 600m
    memory: 2Gi

  http:
    initialdelayseconds: 15
    timeoutseconds: 2
    periodseconds: 15

  rules:
    host: javawebapp.com
    serviceport: 80

  autoscaling:
    minReplicas: 1
    maxReplicas: 3
    targetaverageutilization: 70

  image:
    repository: 582461434625.dkr.ecr.ap-south-1.amazonaws.com/java-web-app
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"

  service:
    type: ClusterIP
    port: 80
    targetport: 8080
	
	
	   deployment.yaml
	   ---------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.applicationname.meta.name }}
  namespace: {{ .Values.applicationname.meta.namespace }}
  labels:
    app: {{ .Values.applicationname.meta.label }}
spec:
  replicas: {{ .Values.applicationname.replicaCount }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: {{ .Values.applicationname.rollingupdate.maxunavailable }}
      maxSurge: {{ .Values.applicationname.rollingupdate.maxsurge }}
  minReadySeconds: {{ .Values.applicationname.rollingupdate.minreadyseconds }}
  selector:
    matchLabels:
      app: {{ .Values.applicationname.meta.label }}
  template:
    metadata:
      name: {{ .Values.applicationname.meta.name }}
      labels:
        app: {{ .Values.applicationname.meta.label }}
    spec:
      containers:
        - name: {{ .Values.applicationname.meta.name }}
          image: "{{ .Values.applicationname.image.repository }}:{{ .Values.applicationname.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.applicationname.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.applicationname.ports.containerport}}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: {{ .Values.applicationname.httpget.port }}
            initialDelaySeconds: {{ .Values.applicationname.http.initialdelayseconds }}
            timeoutSeconds: {{ .Values.applicationname.http.timeoutseconds }}
            periodSeconds: {{ .Values.applicationname.http.periodseconds }}
          readinessProbe:
            httpGet:
              path: /
              port: {{ .Values.applicationname.httpget.port }}
            initialDelaySeconds: {{ .Values.applicationname.http.initialdelayseconds }}
            timeoutSeconds: {{ .Values.applicationname.http.timeoutseconds }}
            periodSeconds: {{ .Values.applicationname.http.periodseconds }}
          resources:
            requests:
              cpu: {{ .Values.applicationname.requests.cpu }}
              memory: {{ .Values.applicationname.requests.memory }}
            limits:
              cpu: {{ .Values.applicationname.limits.cpu }}
              memory: {{ .Values.applicationname.limits.memory }}
		
		
		  hpa.yaml
		  --------
		  
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Values.applicationname.meta.name }}
  namespace: {{ .Values.applicationname.meta.namespace }}
  labels:
    app: {{ .Values.applicationname.meta.label }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Values.applicationname.meta.name }}
  minReplicas: {{ .Values.applicationname.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.applicationname.autoscaling.maxReplicas }}
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 70
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: 70


       service.yaml
       ------------	   

apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.applicationname.meta.name }}
  namespace: {{ .Values.applicationname.meta.namespace }}
  labels:
    app: {{ .Values.applicationname.meta.label }}
spec:
  type: {{ .Values.applicationname.service.type }}
  ports:
    - port: {{ .Values.applicationname.service.port }}
      targetPort: {{ .Values.applicationname.service.targetport }}
      protocol: TCP
      name: http

  selector:
    app: {{ .Values.applicationname.meta.label }}
	
	
	 ingress.yaml
	 ------------
	 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: {{ .Values.applicationname.meta.namespace}}
  name: {{ .Values.applicationname.meta.name }}
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: efk
  name: kibana-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
  - host: <DomainName>
    http:
      paths:
      - backend:
          serviceName: {{ .Values.applicationname.meta.name }}
          servicePort: {{ .Values.applicationname.rules.serviceport }}
        path: /*

(or)

spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              serviceName: {{ .Values.applicationname.meta.name }}
              servicePort: {{ .Values.applicationname.rules.serviceport }}

	 
4) Remove extra files other than above mentioned files except 'Charts'

5) Install the helm chart with the application name and chart name

           helm install <applicationnameAny> <chartname>
		   
		   
		   
Metrics Server setup:
---------------------

1) Check the metrics for the pods

kubectl top po -A

2) Latest Metrics Server release can be installed by running below command

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

3) Now check the metrics for the pods

kubectl top po -A

or

If not able to install metrics server using above method, then follow below steps:

1) Check the metrics for the pods

kubectl top po -A

2) Create a file(Ex: metrics-server.yaml) and paste the below content in the file

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: system:aggregated-metrics-reader
rules:
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    k8s-app: metrics-server
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        image: k8s.gcr.io/metrics-server/metrics-server:v0.5.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          periodSeconds: 10
        name: metrics-server
        ports:
        - containerPort: 443
          name: https
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 20
          periodSeconds: 10
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      volumes:
      - emptyDir: {}
        name: tmp-dir
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  labels:
    k8s-app: metrics-server
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
  version: v1beta1
  versionPriority: 100


3) Run the below command to install the metrics server

Kubectl apply -f mertics-server.yaml

4) Check the metrics server is installed and is in running state

Kubectl get po -n kube-system

5) Now check the metrics for the pods

kubectl top po -A
		   
		   

Cluster Autoscaler setup:
-------------------------

https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html

https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

1) Create a file(cluster-autoscalar.yaml). Change the cluster name at last in the configuration file.

---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.17.3
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<YOUR CLUSTER NAME>
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt #/etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
			
2) Run the below command to apply the create file after changing cluster name in file.

          kubectl apply -f cluster-autoscalar.yaml
		  
		  
3) Increase the maximum number of nodes in the nodegroup. By default it will have 1 as maxmium nodes in nodegroup.(if we created cluster using eksctl) 


4) Change the maxmium pods scaleup in hpa.yaml or update hpa in cluster. Later change the maximum to normal value.

          kubectl edit hpa <hpaname> -n <namespace> 
		  

5) Check if the cluster is scaling up and down after application deployed. Scaleup using below command.
     
	      kubectl scale deploy <deploymentname> --replicas 20 -n <namespace>







EKS Commandas:


aws eks update-kubeconfig --name "dev-zikzuk-app-cluster" --region "ap-south-1"


for auth cluster log in :
aws eks update-kubeconfig --name "dev-zikzuk-auth-cluster" --region "ap-south-1"


kubectl config view # Show Merged kubeconfig settings.
kubectl apply -f ./my-manifest.yaml # create resource(s)
kubectl config use-context my-cluster-name # set the default context to my-cluster-name
kubectl create job hello --image=busybox:1.28 -- echo "Hello World"

kubectl explain pods # get the documentation for pod manifests
kubectl get services # List all services in the namespace
kubectl get pods --all-namespaces # List all pods in all namespaces
kubectl get pods -o wide # List all pods in the current namespace, with more details
kubectl get deployment my-dep # List a particular deployment
kubectl get pods # List all pods in the namespace
kubectl get pod my-pod -o yaml # Get a pod's YAML

kubectl get pods --show-labels # Show labels for all pods (or any other Kubernetes object that supports labelling)
kubectl expose rc nginx --port=80 --target-port=8000 # Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000
kubectl delete -f ./pod.json # Delete a pod using the type and name specified in pod.json
kubectl logs my-pod # dump pod logs (stdout)
kubectl logs deploy/my-deployment # dump Pod logs for a Deployment (single-container case)
kubectl cluster-info # Display addresses of the master and services

kubectl describe pod pdd-name
kubectl delete pod pod-name
kubectl logs podname


helm install deploymentname   helm-name  -n namespace

helm upgrade -i deploymentname   helm-name  -n namespace
helm uninstall deploymentname -n namespace